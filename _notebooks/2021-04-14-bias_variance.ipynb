{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df4ac58",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e1ff9",
   "metadata": {},
   "source": [
    "Bias-variance tradeoff is an essential concept in machine learning and statistics. It reveals the dilema between overfitting and underfitting in the process of minimizing generalization error. In this blog post, we discuss bias-variance tradeoff in the context of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd148412",
   "metadata": {},
   "source": [
    "## Background: supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a5639",
   "metadata": {},
   "source": [
    "### Training-testing workflow\n",
    "\n",
    "In a typical supervised learning problem, we have independent and identically distributed (i.i.d.) data points, where each data point is in the form of predictor(\"$x$\")-outcome(\"$y$\") pairs: $D = \\{(x_1, y_1), (x_2, y_2), \\ldots\\}$. The objective of supervised learning is to learn a mapping $\\hat{\\mu}: \\mathcal{X} \\mapsto \\mathcal{Y}$ from a training data set. Furthermore, if we focus on the mapping $\\hat{\\mu}$'s ability to predict $y$ given new data $x$, we may evaluate $\\hat{\\mu}$ with some loss function on a test data set (the result is also known as **prediction error** or **test error**). The above workflow can be summarised as:\n",
    "$$\n",
    "D_{\\textrm{training}} \\quad \\longrightarrow \\quad \\hat{\\mu}=F(D_{\\textrm{training}}) \\quad \\longrightarrow \\quad \\operatorname{PE} = \\mathcal{L}(\\hat{\\mu}; D_{\\textrm{test}})\n",
    "$$\n",
    "Here $F$ is the learning algorithm, and $\\mathcal{L}$ is a pre-defined loss function to evaluate the performance of a mapping $\\hat{\\mu}$ on a separate test data set.\n",
    "\n",
    "We usually use a loss function with additive property, such that the test error can be written as the summation over losses on each data point. That is:\n",
    "$$\\begin{equation}\n",
    "\\mathcal{L}(\\hat{\\mu}; D_{\\textrm{test}}) = \\frac{1}{n_{\\textrm{test}}} \\sum_{(x_i, y_i) \\in D_{\\textrm{test}}} \\mathcal{L}\\left(\\hat{\\mu}; (x_i, y_i)\\right)\n",
    "\\end{equation}$$\n",
    "\n",
    "### Generalization error\n",
    "We can see that, the prediction error $\\operatorname{PE}$ can be viewed as the result of Monte Carlo simulation. From a theoretical perspective, we hope to reduce sampling error by increasing the size of test. For any given $\\hat{\\mu}$, the test error converges almost surely to its expectation by the law of large numbers:\n",
    "$$\\begin{equation}\n",
    "\\operatorname{PE} = \\mathcal{L}(\\hat{\\mu}; D_{\\textrm{test}}) \\rightarrow_{\\textrm{a.s.}} \\operatorname{EPE} \\quad \\textrm{as} \\quad n_{\\textrm{test}} \\rightarrow \\infty\n",
    "\\end{equation}$$\n",
    "Here $\\operatorname{EPE} := \\mathbb{E} \\left[\\mathcal{L}\\left(\\hat{\\mu}; (X_{\\textrm{test}}, Y_{\\textrm{test}})\\right) \\vert \\hat{\\mu}\\right]$ is the **expected predictor error**. This concept is often seen in machine learning theory.\n",
    "\n",
    "Furthermore, if we are interested in evaluating the performance of the learning algorithm $F$ (instead of the learned mapping $\\hat{\\mu}$), we may take an extra expectation over the training set space. The result is widely known as the **generalization error**:\n",
    "$$\\begin{equation}\n",
    "\\operatorname{GE}_F := \\mathbb{E} \\left[\\mathcal{L}\\left(\\hat{\\mu}; (X_{\\textrm{test}}, Y_{\\textrm{test}})\\right)\\right] = \\mathbb{E} \\left[\\mathcal{L}\\left(F(D_{\\textrm{training}}); (X_{\\textrm{test}}, Y_{\\textrm{test}})\\right)\\right]\n",
    "\\end{equation}$$\n",
    "\n",
    "The generalization error can be understood as the limitting result of the following procedure:\n",
    "1. Sample a training set $D_{\\textrm{training}}$ of size $n_{\\textrm{training}}$.\n",
    "2. Apply learning algorithm and get $\\hat{\\mu} = F(D_{\\textrm{training}})$.\n",
    "3. Sample a test set $D_{\\textrm{test}}$ of infinitely large size $n_{\\textrm{test}} \\rightarrow \\infty$.\n",
    "4. Compute test error $\\operatorname{PE} = \\mathcal{L}(\\hat{\\mu}; D_{\\textrm{test}})$.\n",
    "5. Repeat procedure 1-4 infinitely many times and get $\\operatorname{GE}_F = \\overline{\\operatorname{PE}}$.\n",
    "\n",
    "Note that $\\operatorname{GE}_F$ depends on the training set size $n_{\\textrm{training}}$, which helps answering questions like \"How does the performance change as I increase sample size?\". In the following section, we discuss generalization error in the context of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae90d7",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918849a0",
   "metadata": {},
   "source": [
    "### Mean squared error\n",
    "\n",
    "The mean squared error loss is popular because it is easy to understand and mathematically convenient. It is defined as:\n",
    "$$\\begin{equation}\n",
    "\\mathcal{L}(\\hat{\\mu}; D) := \\frac{1}{n} \\sum_{(x, y) \\in D} \\mathcal{L}\\left(\\hat{\\mu}; (x, y)\\right) = \\frac{1}{n} \\sum_{(x, y) \\in D} \\left(y - \\hat{\\mu}(x)\\right)^2\n",
    "\\end{equation}$$\n",
    "Alghouth the outcome $y$ here is assumed to be real-valued, i.e. $y \\in \\mathbb{R}$, this squared error loss easily generalizes to multivariate regression case ($y \\in \\mathbb{R}^m$) using L2 norm: $\\lVert y - \\hat{\\mu}(x) \\rVert_2^2$.\n",
    "\n",
    "Under mean squared error loss, the oracale mapping (the one that minimizes expected loss) is $\\mu(x) = \\mathbb{E}(Y|x)$, which can be shown as follows:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}\\left[\\mathcal{L}(\\mu; D) \\vert \\mu\\right] &= \\mathbb{E} \\left[\\left(Y - \\mu(X)\\right)^2\\right]\\\\\n",
    "&= \\int_{\\mathcal{X}} \\mathbb{E} \\left[\\left(Y - \\mu(X)\\right)^2 \\vert x\\right] p(x) \\,\\mathrm{d}x\\\\\n",
    "&= \\int_{\\mathcal{X}} \\left[\\mathbb{E} \\left(Y^2 \\vert x\\right) - 2 \\mu(x) \\mathbb{E} \\left(Y \\vert x\\right) + \\mu(x)^2\\right] p(x) \\,\\mathrm{d}x\\\\\n",
    "\\end{align}$$\n",
    "The integral reaches minimum if and only if $\\mu(x) = \\mathbb{E} \\left(Y \\vert x\\right)$. This oracle mapping is only a theoretical concept, as the conditional expectation $\\mathbb{E} \\left(Y \\vert x\\right)$ is unknown in real-world problems. The objective of regression method is to learn an estimate $\\hat{\\mu}$ from training set.\n",
    "\n",
    "### Bias-variance decomposition\n",
    "\n",
    "The generalization error of regression under mean squared error loss can be written as:\n",
    "$$\\begin{align}\n",
    "\\operatorname{GE} &= \\mathbb{E} \\left[\\left(Y - \\hat{\\mu}(X)\\right)^2\\right]\\\\\n",
    "&= \\mathbb{E} \\left\\{\\mathbb{E} \\left[\\left(Y - \\hat{\\mu}(X)\\right)^2 \\big\\vert X\\right]\\right\\}\\\\\n",
    "&= \\mathbb{E} \\left\\{\\mathbb{E} \\left[\\left(Y - \\mu(X) + \\mu(X) - \\hat{\\mu}(X)\\right)^2 \\big\\vert X\\right]\\right\\}\\\\\n",
    "&= \\mathbb{E} \\left(\\mathrm{I} + \\mathrm{II} + \\mathrm{III}\\right)\n",
    "\\end{align}$$\n",
    "Here we decompose the conditional expectation into the following three parts.\n",
    "\n",
    "**Intrisic variance**:\n",
    "$$\\begin{equation}\n",
    "\\mathrm{I} = \\mathbb{E}\\left[\\left(Y - \\mu(X)\\right)^2 \\big\\vert X\\right] = \\operatorname{Var}\\left(Y \\vert X\\right) = \\sigma_X^2\n",
    "\\end{equation}$$\n",
    "\n",
    "**Interaction term**:\n",
    "$$\\begin{align}\n",
    "\\mathrm{II} &= 2\\mathbb{E}\\left[\\left(Y - \\mu(X)\\right) \\cdot \\left(\\mu(X) - \\hat{\\mu}(X)\\right) \\big\\vert X\\right]\\\\\n",
    "&= 2 \\left(\\mu(X) - \\hat{\\mu}(X)\\right) \\cdot \\mathbb{E}\\left[\\left(Y - \\mu(X)\\right) \\big\\vert X\\right]\\\\\n",
    "&= 2 \\left(\\mu(X) - \\hat{\\mu}(X)\\right) \\cdot 0\\\\\n",
    "&= 0\n",
    "\\end{align}$$\n",
    "\n",
    "**Bias-variance decomposition**:\n",
    "$$\\begin{align}\n",
    "\\mathrm{III} &= \\mathbb{E}\\left[\\left(\\mu(X) - \\hat{\\mu}(X)\\right)^2 \\big\\vert X\\right]\\\\\n",
    "&=\\mathbb{E}\\left[\\left(\\mu(X) - \\mathbb{E}\\left(\\hat{\\mu}(X)\\right) + \\mathbb{E}\\left(\\hat{\\mu}(X)\\right) - \\hat{\\mu}(X)\\right)^2 \\big\\vert X\\right]\n",
    "\\end{align}$$\n",
    "Note that the expectation $\\mathbb{E}\\left(\\hat{\\mu}(X)\\right)$ is taken over the training set space (this is where the random fluctuation of $\\hat{\\mu}$ comes from). Again by decomposition:\n",
    "$$\\begin{equation}\n",
    "\\mathrm{III} = \\operatorname{Bias}_X^2 + \\operatorname{Var}_X\n",
    "\\end{equation}$$\n",
    "where $\\operatorname{Bias}_x = \\mathbb{E}\\left[\\hat{\\mu}(x)\\right] - \\mu(x)$ is the expected difference between $\\hat{\\mu}(x)$ and $\\mu(x)$, and $\\operatorname{Var}_x = \\operatorname{Var}\\left[\\hat{\\mu}(x)\\right]$ is the variance of $\\hat{\\mu}(x)$ caused by training set.\n",
    "\n",
    "Putting these all together, we have\n",
    "$$\\begin{equation}\n",
    "\\operatorname{GE} = \\mathbb{E}\\left[\\sigma_X^2 + \\operatorname{Bias}_X^2 + \\operatorname{Var}_X\\right]\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963dbffe",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674e6bf",
   "metadata": {},
   "source": [
    "In the previous section, we showed that the generalization error of regression can be decomposed into three parts:\n",
    "\n",
    "* $\\mathbb{E}\\left[\\sigma_X^2\\right]$: intrisic variance of $Y \\vert X$, averaged over the distribution of $X$. This is part of the data generator and there is nothing we can do to change it.\n",
    "* $\\mathbb{E}\\left[\\operatorname{Bias}_X^2\\right]$: (averaged) squared bias of estimator $\\hat{\\mu}(x)$. A more complicated model can introduce more flexibility in fitting $\\mu$ and reduce the bias term.\n",
    "* $\\mathbb{E}\\left[\\operatorname{Var}_X\\right]$: (averaged) variance of estimator $\\hat{\\mu}(x)$. A larger sample size in training set or a simpler model can reduce the variance term.\n",
    "\n",
    "With a fixed training set, there is a tradeoff between bias and variance, which is tuned by the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147334d6",
   "metadata": {},
   "source": [
    "## Obsolete materials (please review and delete before publish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11dc9f",
   "metadata": {},
   "source": [
    "Regresssion\n",
    "\n",
    "$$\\begin{equation}\\mu(x) = \\mathbb{E}(Y|X)\\end{equation}$$\n",
    "\n",
    "Mean Square Loss\n",
    "$$\\begin{equation}\\mathcal{L} = \\mathbb{E}[(Y-\\hat{\\mu}(x))^2]\\end{equation}$$\n",
    "Conditional Loss\n",
    "$$\\begin{equation}\\mathcal{L}(x) = \\mathbb{E}[(Y-\\hat{\\mu}(x))^2|x]\\end{equation}$$\n",
    "$$\\begin{equation}\\mathcal{L} = \\mathbb{E}[Loss(x)]\\end{equation}$$\n",
    "\n",
    "Bias given x\n",
    "$$\\begin{equation}bias(x) = \\mathbb{E}[\\hat{\\mu}(x)-\\mu(x)|x]\\end{equation}$$\n",
    "Variance given x\n",
    "$$\\begin{equation}var(x) = \\operatorname{Var}[\\hat{\\mu}(x)|x]\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98161d0",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\mathbb{E}[(Y-\\hat{\\mu}(x))^2|x]\n",
    "&= \\mathbb{E}[(Y-\\mu(x)+\\mu(x)-\\hat\\mu(x))^2|x] \\\\\n",
    "&= \\mathbb{E}[(Y-\\mu(x))^2|x] + \\mathbb{E}[(\\mu(x)-\\hat\\mu(x))^2|x] +\n",
    "2\\mathbb{E}[(Y-\\mu(x))(\\mu(x)-\\hat\\mu(x))|x]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b1651",
   "metadata": {},
   "source": [
    "Intrinsic Variance\n",
    "$$\\begin{equation}\\sigma_x^2 = \\mathbb{E}[(Y-\\mu(x))^2|x]\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a60fb0",
   "metadata": {},
   "source": [
    "Zero\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[(Y-\\mu(x))(\\mu(x)-\\hat\\mu(x))|x]\n",
    "&=\\mathbb{E}[(Y-\\mathbb{E}(Y|X))(\\mu(x)-\\hat\\mu(x))|x]\\\\\n",
    "&=0\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a97978",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "\\mathbb{E}[(\\mu(x)-\\hat\\mu(x))^2|x]\n",
    "&= \\mathbb{E}[(\\mu(x)-\\mathbb{E}[\\mu(x)]+\\mathbb{E}[\\mu(x)]-\\hat\\mu(x))^2|x] \\\\\n",
    "&= \\mathbb{E}[(\\mu(x)-\\mathbb{E}[\\mu(x)])^2|x] +\n",
    "\\mathbb{E}[(\\mathbb{E}[\\mu(x)]-\\hat\\mu(x))^2]  \\\\\n",
    "& + 2\\mathbb{E}[(\\mu(x)-\\mathbb{E}[\\mu(x)])(\\mathbb{E}[\\mu(x)]-\\hat\\mu(x))|x] \\\\\n",
    "&= bias^2(x) + var(x)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef1619",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\\mathcal{L}(x)=\\sigma_x^2 + bias^2(x) + var(x)\\end{equation}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "623px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
